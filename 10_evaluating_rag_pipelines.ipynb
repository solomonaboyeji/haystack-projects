{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAG pipelines\n",
    "\n",
    "This section will be divided into 2:\n",
    "1. Part A: Deep Explanation\n",
    "2. Part B: Deep Explanation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Deep Explanation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haystack provides a wide range of Evaluators which can perform 2 types of evaluations:\n",
    "\n",
    "1. Model-Based Evaluation\n",
    "2. Statistical Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model-Based Evaluation\n",
    "\n",
    "Model-based evaluation uses a language model to check the results of a Pipeline using a  Language Model to check the results of a Pipeline.\n",
    "\n",
    "\n",
    "##### Using LLM for Evaluation\n",
    "A golden large language model will be used for this evaluation. The golden large language model such as OpenAI's GPT models, GPT-4, is utilize to evaluate a RAG pipeline by providing it with the Pipeline's results and sometimes additional information, along with a prompt that outlines the evaluation criteria. \n",
    "This does not need labels for the outputs, and it is easy to use. \n",
    "\n",
    "The method of using LLM as an evaluator is very flexible as it exposes a number of metrics to us. Each metrics is ultimately a well-crafted prompt describing to the LLM how to evaluate and score results. \n",
    "\n",
    "Common Metrics includes:\n",
    "\n",
    "1. Faithfulness\n",
    "2. Context Relevance \n",
    "\n",
    "##### Small Cross-Encoder Models for Evaluation\n",
    "Alongside LLMs for evaluation, we can use small cross-encoder models. These models can calculate, for example , semantic answer similarity. In contrast to metrics based on LLMs, as the metrics based on smaller models don't require an API key of a model provider.\n",
    "\n",
    "This method is faster and cheaper to run but it is less flexible in terms of what aspect you can evaluate. You can only evaluate what the small model was trained to evaluate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-Based Evaluation Pipelines in Haystack\n",
    "\n",
    "There are two ways of performing model-based evaluation in Haystack, both of which leverage Pipelien and Evaluator Components\n",
    "\n",
    "1. Create and run an Evaluation Pipeline independently. This means you will have to provide the required inputs to the evaluation Pipeline manually. This is recommend because we can store the results of our RAG pipeline and try out different evaluation metrics afterward without needing to re-run the RAG pipeline every time.\n",
    "\n",
    "2. Add Evaluator Component to the end of the RAG pipeline. This means we run both the RAG Pipeline and the Evaluation on of it in a single pipeline.run() call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model-based Evaluation of Retrieved Documents\n",
    "\n",
    "##### ContextRelevantEvaluator\n",
    "This evaluator uses an LLM to evaluate whether contexts are relevant to a question. It does not require ground truth labels.\n",
    "\n",
    "The component breaks up the context into multiple statements and checks whether each statement is relevant for answering a question. The final score for the context relevance is a number from 0.0 to 1.0 and represents the proportion of statements that are relevant to the provided question.\n",
    "\n",
    "You can pass an example to the evaluator which are sent as few-prompts to the LLM\n",
    "\n",
    "```\n",
    "[{\n",
    "\t\"inputs\": {\n",
    "\t\t\"questions\": \"What is the capital of Italy?\", \"contexts\": [\"Rome is the capital of Italy.\"],\n",
    "\t},\n",
    "\t\"outputs\": {\n",
    "\t\t\"statements\": [\"Rome is the capital of Italy.\", \"Rome has more than 4 million inhabitants.\"],\n",
    "\t\t\"statement_scores\": [1, 0],\n",
    "\t},\n",
    "}]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "A. On its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What makes both Python and Javascript excellent?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[0.5]\n",
      "[{'statements': ['Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.', 'Javascript and Python both have received a lot backlashes but yet keeps waxing strong.'], 'statement_scores': [1, 0], 'score': 0.5}]\n",
      "\n",
      "\n",
      "\n",
      "Question:  Who created the Python Language\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[0.5]\n",
      "[{'statements': ['Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language.', 'Javascript and Python both have received a lot backlashes but yet keeps waxing strong.'], 'statement_scores': [1, 0], 'score': 0.5}]\n",
      "\n",
      "\n",
      "\n",
      "Question:  What are people's feelings towards Javascript?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[1.0]\n",
      "[{'statements': ['Javascript and Python both have received a lot backlashes but yet keeps waxing strong.'], 'statement_scores': [1], 'score': 1.0}]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from haystack.components.evaluators import ContextRelevanceEvaluator\n",
    "\n",
    "questions = ['What makes both Python and Javascript excellent?', 'Who created the Python Language', \"What are people's feelings towards Javascript?\"]\n",
    "contexts = [\n",
    "    [\n",
    "        'Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.',\n",
    "        \"Javascript and Python both have received a lot backlashes but yet keeps waxing strong.\"\n",
    "    ]\n",
    "]\n",
    "for question in questions:\n",
    "    print(\"Question: \", question)\n",
    "    # OpenAI is the only supported model\n",
    "    evaluator = ContextRelevanceEvaluator(raise_on_failure=True)\n",
    "    result = evaluator.run(questions=[question], contexts=contexts)\n",
    "\n",
    "    print(result['score'])\n",
    "    print(result['individual_scores'])\n",
    "    \n",
    "    # Notice the statement_score\n",
    "    print(result['results'])\n",
    "    print(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. In a Pipeline\n",
    "\n",
    "In this example, we use the ContextRelevanceEvaluator and the FaithfulnessEvaluator together in a pipeline to evaluate responses and context (in the content of documents) recieved by a RAG pipeline based on the provided questionst.\n",
    "\n",
    "This is an example of how we can run multiple metrics after we receive the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Individual Scores\n",
      "context_relevance_evaluator  =>  [1.0]\n",
      "Statement:\n",
      "['Python, created by Guido van Rossum in the late 1980s.']\n",
      "faithfulness_evaluator  =>  [0.5]\n",
      "Statement:\n",
      "['Python is a high-level general-purpose programming language.', 'Python was created by Guido van Rossum in the late 1980s.']\n",
      "\n",
      "Score\n",
      "context_relevance_evaluator  =>  1.0\n",
      "Statement:\n",
      "['Python, created by Guido van Rossum in the late 1980s.']\n",
      "faithfulness_evaluator  =>  0.5\n",
      "Statement:\n",
      "['Python is a high-level general-purpose programming language.', 'Python was created by Guido van Rossum in the late 1980s.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.evaluators import ContextRelevanceEvaluator, FaithfulnessEvaluator\n",
    "\n",
    "pipeline = Pipeline()\n",
    "context_relevance_evaluator = ContextRelevanceEvaluator()\n",
    "faithfulness_evaluator = FaithfulnessEvaluator() # evaluates generated/extracted answers, more on this in the next secion\n",
    "pipeline.add_component(\"context_relevance_evaluator\", context_relevance_evaluator)\n",
    "pipeline.add_component(\"faithfulness_evaluator\", faithfulness_evaluator)\n",
    "\n",
    "questions = [\"Who created the Python Language?\"]\n",
    "contexts = [\n",
    "    [\n",
    "        \"Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.\"\n",
    "    ],\n",
    "]\n",
    "predicted_answers = [\"Python is a high-level general-purpose programming language that was created by George Lucas\"]\n",
    "\n",
    "result = pipeline.run({\n",
    "    \"context_relevance_evaluator\": {\n",
    "        \"questions\": questions, \"contexts\": contexts\n",
    "    },\n",
    "    \"faithfulness_evaluator\": {\n",
    "        \"questions\": questions,\n",
    "        \"contexts\": contexts,\n",
    "        \"predicted_answers\": predicted_answers\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"\\nIndividual Scores\")\n",
    "for evaluator in result:\n",
    "    print(evaluator , \" => \", result[evaluator]['individual_scores'])\n",
    "    print(\"Statement:\")\n",
    "    for ev_result in result[evaluator]['results']:\n",
    "        print(ev_result['statements'])\n",
    "    \n",
    "print(\"\\nScore\")\n",
    "for evaluator in result:\n",
    "    print(evaluator , \" => \", result[evaluator]['score'])\n",
    "    print(\"Statement:\")\n",
    "    for ev_result in result[evaluator]['results']:\n",
    "        print(ev_result['statements'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Model-based Evaluation of Generated or Extracted Answers\n",
    "##### FaithfulnessEvaluator (aka groundedness)\n",
    "\n",
    "This uses an LLM to evaluate whether a generated answer can be inferred from the provided contexts. It does not require ground truth labels.\n",
    "\n",
    "The metric is sometimes called groundedness or hallucination.\n",
    "\n",
    "FaithfulnessEvaluator component can be used to evaluate documents retrieved by a Haystack pipeline, such as RAG pipeline, without ground truth labels.\n",
    "\n",
    "The component splits the generated answer into statements and checks each of them against the provided context, with an LLM. A higher faithfulness score is better, and it indicates that a larger number of statements in the generated answers can be inferred from the contexts. \n",
    "\n",
    "This score can be used to better understand how often and when the Generator in a RAG pipeline hallucinates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "A. On its own\n",
    "\n",
    "An example of using a FaithfulnessEvaluator component to evaluate a predicted answer generated based on a provided question and context. It returned a score of 0.5 because it detects two statements in the answer, of which only one is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5]\n",
      "0.5\n",
      "[{'statements': ['Python is a high-level general-purpose programming language.', 'Python was created by Guido van Rossum in the late 1980s.'], 'statement_scores': [1, 0], 'score': 0.5}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.evaluators import FaithfulnessEvaluator\n",
    "\n",
    "questions = [\"Who created the Python language?\"]\n",
    "contexts = [\n",
    "    [\n",
    "        \"Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.\"\n",
    "    ],\n",
    "]\n",
    "predicted_answers = [\"Python is a high-level general-purpose programming language that was created by George Lucas.\"]\n",
    "evaluator = FaithfulnessEvaluator()\n",
    "result = evaluator.run(\n",
    "        questions=questions, \n",
    "        contexts=contexts, \n",
    "        predicted_answers=predicted_answers\n",
    "    )\n",
    "\n",
    "print(result[\"individual_scores\"])\n",
    "\n",
    "print(result[\"score\"])\n",
    "\n",
    "print(result[\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A. In a Pipeline\n",
    "\n",
    "As shown in the ContextRelevanceEvaluator.\n",
    "Skipping this to avoid excessive usage of credits.\n",
    "\n",
    "// NO CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SASEvaluator (Semantic Answer Similarity)\n",
    "\n",
    "SASEvaluator evaluates answers predicted by pipelines using ground truth labels. It checks the semantic similarity of a predicted answer and the ground truth answer using a fine-tuned language model. The metric is called Semantic Answer Similarity.\n",
    "\n",
    "The evaluator uses a bi-encoder or a cross-encoder model. By default it uses the `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` mode.\n",
    "\n",
    "NOTE: Only one predicted answer is compared to one ground truth answer at a time. The component does not support multiple ground truth answers for the same question or multiple answers predicted for the same question.\n",
    "\n",
    "https://arxiv.org/abs/2108.06130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "A. On its own\n",
    "\n",
    "The example below compares two answers and compare them to ground truth answers. We need to call the `warm_up()` before `run()` to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:919: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:173: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v3 of SentenceTransformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999999403953552, 0.5174765586853027]\n",
      "0.758738249540329\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.evaluators import SASEvaluator\n",
    "\n",
    "# model is from huggingface\n",
    "sas_evaluator = SASEvaluator(model=\"sentence-transformers/paraphrase\")\n",
    "sas_evaluator.warm_up()\n",
    "result = sas_evaluator.run(\n",
    "    ground_truth_answers=[\"Berlin\", \"Paris\"],\n",
    "    predicted_answers=[\"Berlin\", \"Lyon\"]\n",
    ")\n",
    "print(result['individual_scores'])\n",
    "\n",
    "print(result['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A. In a Pipeline\n",
    "\n",
    "Below is an example where we use an `AnswerExactMatchEvaluator` and a `SASEvaluator` in a pipeline to evaluate two answers and compare them to a ground truth answesr.\n",
    "\n",
    "Running a pipeline instead of the individual components simplifies calculating more than one metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:919: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:173: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v3 of SentenceTransformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Individual Scores\n",
      "[1, 0]\n",
      "[0.9999999403953552, 0.5174765586853027]\n",
      "\n",
      "Score\n",
      "0.5\n",
      "0.758738249540329\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.evaluators import AnswerExactMatchEvaluator, SASEvaluator\n",
    "\n",
    "pipeline = Pipeline()\n",
    "em_evaluator = AnswerExactMatchEvaluator()\n",
    "sas_evaluator = SASEvaluator()\n",
    "\n",
    "pipeline.add_component(\"em_evaluator\", em_evaluator)\n",
    "pipeline.add_component(\"sas_evaluator\", sas_evaluator)\n",
    "\n",
    "\n",
    "ground_truth_answers = [\"Berlin\", \"Paris\"]\n",
    "predicted_answers = [\"Berlin\", \"Lyon\"]\n",
    "\n",
    "result = pipeline.run({\n",
    "    \"em_evaluator\": {\n",
    "        \"ground_truth_answers\": ground_truth_answers,\n",
    "        \"predicted_answers\": predicted_answers\n",
    "    },\n",
    "    \"sas_evaluator\": {\n",
    "        \"ground_truth_answers\": ground_truth_answers,\n",
    "        \"predicted_answers\": predicted_answers,\n",
    "    }\n",
    "})\n",
    "\n",
    "\n",
    "print(\"\\nIndividual Scores\") \n",
    "\n",
    "for evaluator in result:\n",
    "    print(result[evaluator]['individual_scores'])\n",
    "   \n",
    "print(\"\\nScore\") \n",
    "\n",
    "for evaluator in result:\n",
    "    print(result[evaluator]['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RagasEvaluator\n",
    "\n",
    "RAGAS is a framework that helps you evaluate RAG pipelines. \n",
    "\n",
    "Learn more about RAGAS here: https://docs.ragas.io/en/latest/index.html\n",
    "\n",
    "Supported Metrics\n",
    "\n",
    "- `ANSWER_CORRECTNESS`: grades the accuracy of the generated answer when compared to the ground truth.\n",
    "- `FAITHFULNESS`: grades how factual the generated response was.\n",
    "- `ANSWER_SIMILARITY`: grades how similar the generated answer is to the ground truth answer specified.\n",
    "- `CONTEXT_PRECISION`: grades if the answer has any additional irrelevant information for the question asked.\n",
    "- `CONTEXT_UTILIZATION`: grade to what extent the generated answer uses the provided context\n",
    "- `CONTEXT_RECALL`: grades how complete the generated response was for the question specified\n",
    "- `ASPECT_CRITIQUE`: grades generated answers based on custom aspects on a binary scale\n",
    "- `CONTEXT_RELEVANCY`: grades how irrelevant the provided context was for the question specified\n",
    "- `ANSWER_RELEVANCY`: grades how relevant the generated response is given the question.\n",
    "\n",
    "Models Supported includes:\n",
    "- All GPT models from OpenAI\n",
    "- Google VertexAI Models\n",
    "- Azure OpenAI Models\n",
    "- Amazon Bedrock Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "1. You can use the `RagasEvaluator` while providing correct `metric_params` for the metric you are using.\n",
    "2. Run the `RagasEvaluator`, either on its own or in a pipeline, by providing the expected input for the metric you are using.\n",
    "\n",
    "##### Examples\n",
    "##### Evaluate Context Relevance\n",
    "Create a context-relevance evaluation pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00,  3.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluator': {'results': [[{'name': 'context_relevancy', 'score': 1.0}],\n",
       "   [{'name': 'context_relevancy', 'score': 1.0}]]}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack_integrations.components.evaluators.ragas import RagasEvaluator, RagasMetric\n",
    "\n",
    "pipeline = Pipeline()\n",
    "evaluator = RagasEvaluator(\n",
    "    metric=RagasMetric.CONTEXT_RELEVANCY\n",
    ")\n",
    "pipeline.add_component(\"evaluator\", evaluator)\n",
    "\n",
    "\n",
    "# send this data into the evaluator\n",
    "results = pipeline.run({\n",
    "    \"evaluator\": {\n",
    "        \"questions\": [\n",
    "            \"When was the Rhodes Statue built?\",  # question 1\n",
    "            \"Where is the Pyramid of Giza\" # question 2\n",
    "        ],\n",
    "        \"contexts\": [\n",
    "            [ \"Context for question 1\" ], # context for question 1\n",
    "            [\"Context for question 2\"] # context for question 1\n",
    "        ]\n",
    "    }\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Evaluate Relevance and Aspect Critique\n",
    "Create a context-relevance and aspect critique evaluation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack_integrations.components.evaluators.ragas import RagasEvaluator, RagasMetric\n",
    "\n",
    "pipeline = Pipeline()\n",
    "evaluator_context = RagasEvaluator(\n",
    "    metric=RagasMetric.CONTEXT_PRECISION\n",
    ")\n",
    "evaluator_aspect = RagasEvaluator(\n",
    "    metric=RagasMetric.ASPECT_CRITIQUE,\n",
    "    metric_params={\n",
    "        \"name\": \"valid_for_children\",\n",
    "        \"definition\": \"Is this answer problematic for children?\", \n",
    "        \"strictness\": 3\n",
    "    }\n",
    ")\n",
    "pipeline.add_component(\"evaluator_context\", evaluator_context)\n",
    "pipeline.add_component(\"evaluator_aspect\", evaluator_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:01<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evaluator_aspect': {'results': [[{'name': 'valid_for_children', 'score': 1}],\n",
      "                                  [{'name': 'valid_for_children',\n",
      "                                    'score': 0}]]},\n",
      " 'evaluator_context': {'results': [[{'name': 'context_precision',\n",
      "                                     'score': 0.9999999999}],\n",
      "                                   [{'name': 'context_precision',\n",
      "                                     'score': 0.9999999999}]]}}\n"
     ]
    }
   ],
   "source": [
    "QUESTIONS = [\"Which is the most popular global sport?\", \"Who created the Python language?\"]\n",
    "CONTEXTS = [[\"The popularity of sports can be measured in various ways, including TV viewership, social media presence, number of participants, and economic impact. Football is undoubtedly the world's most popular sport with major events like the FIFA World Cup and sports personalities like Ronaldo and Messi, drawing a followership of more than 4 billion people.\"], \n",
    "                 [\"Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.\"]]\n",
    "RESPONSES = [\"Football is the most popular sport with around 4 billion followers worldwide\", \"Python language was created by Guido van Rossum.\"]\n",
    "GROUND_TRUTHS = [\"Football is the most popular sport\", \"Python language was created by Guido van Rossum.\"]\n",
    "\n",
    "results = pipeline.run({\n",
    "    \"evaluator_context\": {\n",
    "        \"questions\": QUESTIONS, \"contexts\": CONTEXTS, \"ground_truths\": GROUND_TRUTHS\n",
    "    },\n",
    "    \"evaluator_aspect\": {\n",
    "        \"questions\": QUESTIONS, \"contexts\": CONTEXTS, \"responses\": RESPONSES\n",
    "    }\n",
    "})\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DeepEvalEvaluator\n",
    "\n",
    "DeepEval is a simple-to-use, open-source LLM evaluation framework. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs locally on your machine for evaluation.\n",
    "\n",
    "DeepEval gives you additional options of using models on your machine.\n",
    "https://docs.confident-ai.com/docs/metrics-introduction#using-a-custom-llm \n",
    "\n",
    "Integrate into CI/CDs.\n",
    "\n",
    "Supported Metrics:\n",
    "- `ANSWER_RELEVANCY`: grades how relevant the answer was to the question specified\n",
    "- `FAITHFULNESS`: grades how factual the generated response was.\n",
    "- `CONTEXTUAL_PRECISION`: grades if he answer has any additional irrelevant information for the question asked.\n",
    "- `CONTEXTUAL_RECALL`: grades how complete the generated response was for the question specified\n",
    "- `CONTEXTUAL_RELEVANCE`: grades how relevant provided context was for the question specified\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "1. You can use the `DeepEvalEvaluator` while providing correct `metric_params` for the metric you are using.\n",
    "2. Run the `DeepEvalEvaluator`, either on its own or in a pipeline, by providing the expected input for the metric you are using.\n",
    "\n",
    "##### Examples\n",
    "##### Evaluate Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "======================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Metrics Summary\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Metrics Summary\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  - ✅ Faithfulness (score: 0, threshold: 0.0, evaluation model: gpt-4, reason: The score is 0.00 because there \n",
       "were no contradictions found between the actual output and the retrieval context. The actual output perfectly \n",
       "aligned with all the nodes in the retrieval context.)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  - ✅ Faithfulness (score: 0, threshold: 0.0, evaluation model: gpt-4, reason: The score is 0.00 because there \n",
       "were no contradictions found between the actual output and the retrieval context. The actual output perfectly \n",
       "aligned with all the nodes in the retrieval context.)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">For test case:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "For test case:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  - input: When was the Rhodes Statue built?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  - input: When was the Rhodes Statue built?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  - actual output: Response for question 1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  - actual output: Response for question 1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  - expected output: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  - expected output: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  - context: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  - context: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  - retrieval context: ['Context for question 1']\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  - retrieval context: ['Context for question 1']\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "======================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Metrics Summary\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Metrics Summary\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  - ✅ Faithfulness (score: 1.0, threshold: 0.0, evaluation model: gpt-4, reason: The score is 1.00 because there \n",
       "are no contradictions between the actual output and the retrieval context, indicating a perfect alignment.)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  - ✅ Faithfulness (score: 1.0, threshold: 0.0, evaluation model: gpt-4, reason: The score is 1.00 because there \n",
       "are no contradictions between the actual output and the retrieval context, indicating a perfect alignment.)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">For test case:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "For test case:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  - input: Where is the Pyramid of Giza\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  - input: Where is the Pyramid of Giza\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  - actual output: Response for question 2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  - actual output: Response for question 2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  - expected output: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  - expected output: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  - context: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  - context: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  - retrieval context: ['Context for question 2']\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  - retrieval context: ['Context for question 2']\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack_integrations.components.evaluators.deepeval import DeepEvalEvaluator, DeepEvalMetric\n",
    "\n",
    "pipeline = Pipeline()\n",
    "evaluator = DeepEvalEvaluator(\n",
    "    metric=DeepEvalMetric.FAITHFULNESS,\n",
    "    metric_params={\"model\": \"gpt-4\"}\n",
    ")\n",
    "pipeline.add_component(\"evaluator\", evaluator)\n",
    "\n",
    "results = pipeline.run({\n",
    "    \"evaluator\": {\n",
    "        \"questions\": [\"When was the Rhodes Statue built?\", \"Where is the Pyramid of Giza\" ],\n",
    "        \"contexts\": [ [\"Context for question 1\"], [\"Context for question 2\"] ],\n",
    "        \"responses\": [\"Response for question 1\", \"Response for question 2\"]\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Practical Examples\n",
    "\n",
    "\n",
    "We would evaluate RAG pipelines both with model-based and statistical metrics available in Haystack evaluation offering.\n",
    "\n",
    "1. Build a pipeline that answers medical questions based on PubMed data\n",
    "2. Build an evaluation pipeline that makes use of some metrics like Document MRR and Answer Faithfulness \n",
    "3. Run the RAG pipeline and evaluate the output with our evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
