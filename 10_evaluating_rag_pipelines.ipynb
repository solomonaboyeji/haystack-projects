{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAG pipelines\n",
    "\n",
    "This section will be divided into 2:\n",
    "\n",
    "1. Part A: Deep Explanation\n",
    "2. Part B: Deep Explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Deep Explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haystack provides a wide range of Evaluators which can perform 2 types of evaluations:\n",
    "\n",
    "1. Model-Based Evaluation\n",
    "2. Statistical Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model-Based Evaluation\n",
    "\n",
    "Model-based evaluation uses a language model to check the results of a Pipeline using a Language Model to check the results of a Pipeline.\n",
    "\n",
    "##### Using LLM for Evaluation\n",
    "\n",
    "A golden large language model will be used for this evaluation. The golden large language model such as OpenAI's GPT models, GPT-4, is utilize to evaluate a RAG pipeline by providing it with the Pipeline's results and sometimes additional information, along with a prompt that outlines the evaluation criteria.\n",
    "This does not need labels for the outputs, and it is easy to use.\n",
    "\n",
    "The method of using LLM as an evaluator is very flexible as it exposes a number of metrics to us. Each metrics is ultimately a well-crafted prompt describing to the LLM how to evaluate and score results.\n",
    "\n",
    "Common Metrics includes:\n",
    "\n",
    "1. Faithfulness\n",
    "2. Context Relevance\n",
    "\n",
    "##### Small Cross-Encoder Models for Evaluation\n",
    "\n",
    "Alongside LLMs for evaluation, we can use small cross-encoder models. These models can calculate, for example , semantic answer similarity. In contrast to metrics based on LLMs, as the metrics based on smaller models don't require an API key of a model provider.\n",
    "\n",
    "This method is faster and cheaper to run but it is less flexible in terms of what aspect you can evaluate. You can only evaluate what the small model was trained to evaluate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-Based Evaluation Pipelines in Haystack\n",
    "\n",
    "There are two ways of performing model-based evaluation in Haystack, both of which leverage Pipelien and Evaluator Components\n",
    "\n",
    "1. Create and run an Evaluation Pipeline independently. This means you will have to provide the required inputs to the evaluation Pipeline manually. This is recommend because we can store the results of our RAG pipeline and try out different evaluation metrics afterward without needing to re-run the RAG pipeline every time.\n",
    "\n",
    "2. Add Evaluator Component to the end of the RAG pipeline. This means we run both the RAG Pipeline and the Evaluation on of it in a single pipeline.run() call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model-based Evaluation of Retrieved Documents\n",
    "\n",
    "##### ContextRelevantEvaluator\n",
    "\n",
    "This evaluator uses an LLM to evaluate whether contexts are relevant to a question. It does not require ground truth labels.\n",
    "\n",
    "The component breaks up the context into multiple statements and checks whether each statement is relevant for answering a question. The final score for the context relevance is a number from 0.0 to 1.0 and represents the proportion of statements that are relevant to the provided question.\n",
    "\n",
    "You can pass an example to the evaluator which are sent as few-prompts to the LLM\n",
    "\n",
    "```\n",
    "[{\n",
    "\t\"inputs\": {\n",
    "\t\t\"questions\": \"What is the capital of Italy?\", \"contexts\": [\"Rome is the capital of Italy.\"],\n",
    "\t},\n",
    "\t\"outputs\": {\n",
    "\t\t\"statements\": [\"Rome is the capital of Italy.\", \"Rome has more than 4 million inhabitants.\"],\n",
    "\t\t\"statement_scores\": [1, 0],\n",
    "\t},\n",
    "}]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "A. On its own\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What makes both Python and Javascript excellent?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[0.5]\n",
      "[{'statements': ['Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.', 'Javascript and Python both have received a lot backlashes but yet keeps waxing strong.'], 'statement_scores': [1, 0], 'score': 0.5}]\n",
      "\n",
      "\n",
      "\n",
      "Question:  Who created the Python Language\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[1.0]\n",
      "[{'statements': ['Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language.'], 'statement_scores': [1], 'score': 1.0}]\n",
      "\n",
      "\n",
      "\n",
      "Question:  What are people's feelings towards Javascript?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[1.0]\n",
      "[{'statements': ['Javascript and Python both have received a lot backlashes but yet keeps waxing strong.'], 'statement_scores': [1], 'score': 1.0}]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.evaluators import ContextRelevanceEvaluator\n",
    "\n",
    "questions = [\n",
    "    \"What makes both Python and Javascript excellent?\",\n",
    "    \"Who created the Python Language\",\n",
    "    \"What are people's feelings towards Javascript?\",\n",
    "]\n",
    "contexts = [\n",
    "    [\n",
    "        \"Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.\",\n",
    "        \"Javascript and Python both have received a lot backlashes but yet keeps waxing strong.\",\n",
    "    ]\n",
    "]\n",
    "for question in questions:\n",
    "    print(\"Question: \", question)\n",
    "    # OpenAI is the only supported model\n",
    "    evaluator = ContextRelevanceEvaluator(raise_on_failure=True)\n",
    "    result = evaluator.run(questions=[question], contexts=contexts)\n",
    "\n",
    "    print(result[\"score\"])\n",
    "    print(result[\"individual_scores\"])\n",
    "\n",
    "    # Notice the statement_score\n",
    "    print(result[\"results\"])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. In a Pipeline\n",
    "\n",
    "In this example, we use the ContextRelevanceEvaluator and the FaithfulnessEvaluator together in a pipeline to evaluate responses and context (in the content of documents) recieved by a RAG pipeline based on the provided questionst.\n",
    "\n",
    "This is an example of how we can run multiple metrics after we receive the context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Individual Scores\n",
      "context_relevance_evaluator  =>  [1.0]\n",
      "Statement:\n",
      "['Python, created by Guido van Rossum in the late 1980s.']\n",
      "faithfulness_evaluator  =>  [0.5]\n",
      "Statement:\n",
      "['Python is a high-level general-purpose programming language.', 'Python was created by Guido van Rossum in the late 1980s.']\n",
      "\n",
      "Score\n",
      "context_relevance_evaluator  =>  1.0\n",
      "Statement:\n",
      "['Python, created by Guido van Rossum in the late 1980s.']\n",
      "faithfulness_evaluator  =>  0.5\n",
      "Statement:\n",
      "['Python is a high-level general-purpose programming language.', 'Python was created by Guido van Rossum in the late 1980s.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.evaluators import (\n",
    "    ContextRelevanceEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline()\n",
    "context_relevance_evaluator = ContextRelevanceEvaluator()\n",
    "faithfulness_evaluator = (\n",
    "    FaithfulnessEvaluator()\n",
    ")  # evaluates generated/extracted answers, more on this in the next secion\n",
    "pipeline.add_component(\"context_relevance_evaluator\", context_relevance_evaluator)\n",
    "pipeline.add_component(\"faithfulness_evaluator\", faithfulness_evaluator)\n",
    "\n",
    "questions = [\"Who created the Python Language?\"]\n",
    "contexts = [\n",
    "    [\n",
    "        \"Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.\"\n",
    "    ],\n",
    "]\n",
    "predicted_answers = [\n",
    "    \"Python is a high-level general-purpose programming language that was created by George Lucas\"\n",
    "]\n",
    "\n",
    "result = pipeline.run(\n",
    "    {\n",
    "        \"context_relevance_evaluator\": {\"questions\": questions, \"contexts\": contexts},\n",
    "        \"faithfulness_evaluator\": {\n",
    "            \"questions\": questions,\n",
    "            \"contexts\": contexts,\n",
    "            \"predicted_answers\": predicted_answers,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nIndividual Scores\")\n",
    "for evaluator in result:\n",
    "    print(evaluator, \" => \", result[evaluator][\"individual_scores\"])\n",
    "    print(\"Statement:\")\n",
    "    for ev_result in result[evaluator][\"results\"]:\n",
    "        print(ev_result[\"statements\"])\n",
    "\n",
    "print(\"\\nScore\")\n",
    "for evaluator in result:\n",
    "    print(evaluator, \" => \", result[evaluator][\"score\"])\n",
    "    print(\"Statement:\")\n",
    "    for ev_result in result[evaluator][\"results\"]:\n",
    "        print(ev_result[\"statements\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model-based Evaluation of Generated or Extracted Answers\n",
    "\n",
    "##### FaithfulnessEvaluator (aka groundedness)\n",
    "\n",
    "This uses an LLM to evaluate whether a generated answer can be inferred from the provided contexts. It does not require ground truth labels.\n",
    "\n",
    "The metric is sometimes called groundedness or hallucination.\n",
    "\n",
    "FaithfulnessEvaluator component can be used to evaluate documents retrieved by a Haystack pipeline, such as RAG pipeline, without ground truth labels.\n",
    "\n",
    "The component splits the generated answer into statements and checks each of them against the provided context, with an LLM. A higher faithfulness score is better, and it indicates that a larger number of statements in the generated answers can be inferred from the contexts.\n",
    "\n",
    "This score can be used to better understand how often and when the Generator in a RAG pipeline hallucinates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "A. On its own\n",
    "\n",
    "An example of using a FaithfulnessEvaluator component to evaluate a predicted answer generated based on a provided question and context. It returned a score of 0.5 because it detects two statements in the answer, of which only one is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5]\n",
      "0.5\n",
      "[{'statements': ['Python is a high-level general-purpose programming language.', 'Python was created by Guido van Rossum in the late 1980s.'], 'statement_scores': [1, 0], 'score': 0.5}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.evaluators import FaithfulnessEvaluator\n",
    "\n",
    "questions = [\"Who created the Python language?\"]\n",
    "contexts = [\n",
    "    [\n",
    "        \"Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.\"\n",
    "    ],\n",
    "]\n",
    "predicted_answers = [\n",
    "    \"Python is a high-level general-purpose programming language that was created by George Lucas.\"\n",
    "]\n",
    "evaluator = FaithfulnessEvaluator()\n",
    "result = evaluator.run(\n",
    "    questions=questions, contexts=contexts, predicted_answers=predicted_answers\n",
    ")\n",
    "\n",
    "print(result[\"individual_scores\"])\n",
    "\n",
    "print(result[\"score\"])\n",
    "\n",
    "print(result[\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. In a Pipeline\n",
    "\n",
    "As shown in the ContextRelevanceEvaluator.\n",
    "Skipping this to avoid excessive usage of credits.\n",
    "\n",
    "// NO CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SASEvaluator (Semantic Answer Similarity)\n",
    "\n",
    "SASEvaluator evaluates answers predicted by pipelines using ground truth labels. It checks the semantic similarity of a predicted answer and the ground truth answer using a fine-tuned language model. The metric is called Semantic Answer Similarity.\n",
    "\n",
    "The evaluator uses a bi-encoder or a cross-encoder model. By default it uses the `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` mode.\n",
    "\n",
    "NOTE: Only one predicted answer is compared to one ground truth answer at a time. The component does not support multiple ground truth answers for the same question or multiple answers predicted for the same question.\n",
    "\n",
    "https://arxiv.org/abs/2108.06130\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "A. On its own\n",
    "\n",
    "The example below compares two answers and compare them to ground truth answers. We need to call the `warm_up()` before `run()` to load the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999999403953552, 0.5174765586853027]\n",
      "0.758738249540329\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.evaluators import SASEvaluator\n",
    "\n",
    "# model is from huggingface\n",
    "sas_evaluator = SASEvaluator()\n",
    "sas_evaluator.warm_up()\n",
    "result = sas_evaluator.run(\n",
    "    ground_truth_answers=[\"Berlin\", \"Paris\"], predicted_answers=[\"Berlin\", \"Lyon\"]\n",
    ")\n",
    "print(result[\"individual_scores\"])\n",
    "\n",
    "print(result[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. In a Pipeline\n",
    "\n",
    "Below is an example where we use an `AnswerExactMatchEvaluator` and a `SASEvaluator` in a pipeline to evaluate two answers and compare them to a ground truth answesr.\n",
    "\n",
    "Running a pipeline instead of the individual components simplifies calculating more than one metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Individual Scores\n",
      "[1, 0]\n",
      "[0.9999999403953552, 0.5174765586853027]\n",
      "\n",
      "Score\n",
      "0.5\n",
      "0.758738249540329\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.evaluators import AnswerExactMatchEvaluator, SASEvaluator\n",
    "\n",
    "pipeline = Pipeline()\n",
    "em_evaluator = AnswerExactMatchEvaluator()\n",
    "sas_evaluator = SASEvaluator()\n",
    "\n",
    "pipeline.add_component(\"em_evaluator\", em_evaluator)\n",
    "pipeline.add_component(\"sas_evaluator\", sas_evaluator)\n",
    "\n",
    "\n",
    "ground_truth_answers = [\"Berlin\", \"Paris\"]\n",
    "predicted_answers = [\"Berlin\", \"Lyon\"]\n",
    "\n",
    "result = pipeline.run(\n",
    "    {\n",
    "        \"em_evaluator\": {\n",
    "            \"ground_truth_answers\": ground_truth_answers,\n",
    "            \"predicted_answers\": predicted_answers,\n",
    "        },\n",
    "        \"sas_evaluator\": {\n",
    "            \"ground_truth_answers\": ground_truth_answers,\n",
    "            \"predicted_answers\": predicted_answers,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nIndividual Scores\")\n",
    "\n",
    "for evaluator in result:\n",
    "    print(result[evaluator][\"individual_scores\"])\n",
    "\n",
    "print(\"\\nScore\")\n",
    "\n",
    "for evaluator in result:\n",
    "    print(result[evaluator][\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RagasEvaluator\n",
    "\n",
    "RAGAS is a framework that helps you evaluate RAG pipelines.\n",
    "\n",
    "Learn more about RAGAS here: https://docs.ragas.io/en/latest/index.html\n",
    "\n",
    "Supported Metrics\n",
    "\n",
    "- `ANSWER_CORRECTNESS`: grades the accuracy of the generated answer when compared to the ground truth.\n",
    "- `FAITHFULNESS`: grades how factual the generated response was.\n",
    "- `ANSWER_SIMILARITY`: grades how similar the generated answer is to the ground truth answer specified.\n",
    "- `CONTEXT_PRECISION`: grades if the answer has any additional irrelevant information for the question asked.\n",
    "- `CONTEXT_UTILIZATION`: grade to what extent the generated answer uses the provided context\n",
    "- `CONTEXT_RECALL`: grades how complete the generated response was for the question specified\n",
    "- `ASPECT_CRITIQUE`: grades generated answers based on custom aspects on a binary scale\n",
    "- `CONTEXT_RELEVANCY`: grades how irrelevant the provided context was for the question specified\n",
    "- `ANSWER_RELEVANCY`: grades how relevant the generated response is given the question.\n",
    "\n",
    "Models Supported includes:\n",
    "\n",
    "- All GPT models from OpenAI\n",
    "- Google VertexAI Models\n",
    "- Azure OpenAI Models\n",
    "- Amazon Bedrock Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "1. You can use the `RagasEvaluator` while providing correct `metric_params` for the metric you are using.\n",
    "2. Run the `RagasEvaluator`, either on its own or in a pipeline, by providing the expected input for the metric you are using.\n",
    "\n",
    "##### Examples\n",
    "\n",
    "##### Evaluate Context Relevance\n",
    "\n",
    "Create a context-relevance evaluation pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00,  3.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluator': {'results': [[{'name': 'context_relevancy', 'score': 1.0}],\n",
       "   [{'name': 'context_relevancy', 'score': 1.0}]]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack_integrations.components.evaluators.ragas import (\n",
    "    RagasEvaluator,\n",
    "    RagasMetric,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline()\n",
    "evaluator = RagasEvaluator(metric=RagasMetric.CONTEXT_RELEVANCY)\n",
    "pipeline.add_component(\"evaluator\", evaluator)\n",
    "\n",
    "\n",
    "# send this data into the evaluator\n",
    "results = pipeline.run(\n",
    "    {\n",
    "        \"evaluator\": {\n",
    "            \"questions\": [\n",
    "                \"When was the Rhodes Statue built?\",  # question 1\n",
    "                \"Where is the Pyramid of Giza\",  # question 2\n",
    "            ],\n",
    "            \"contexts\": [\n",
    "                [\"Context for question 1\"],  # context for question 1\n",
    "                [\"Context for question 2\"],  # context for question 1\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate Relevance and Aspect Critique\n",
    "\n",
    "Create a context-relevance and aspect critique evaluation pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack_integrations.components.evaluators.ragas import (\n",
    "    RagasEvaluator,\n",
    "    RagasMetric,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline()\n",
    "evaluator_context = RagasEvaluator(metric=RagasMetric.CONTEXT_PRECISION)\n",
    "evaluator_aspect = RagasEvaluator(\n",
    "    metric=RagasMetric.ASPECT_CRITIQUE,\n",
    "    metric_params={\n",
    "        \"name\": \"valid_for_children\",\n",
    "        \"definition\": \"Is this answer problematic for children?\",\n",
    "        \"strictness\": 3,\n",
    "    },\n",
    ")\n",
    "pipeline.add_component(\"evaluator_context\", evaluator_context)\n",
    "pipeline.add_component(\"evaluator_aspect\", evaluator_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:01<00:00,  1.70it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:01<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evaluator_aspect': {'results': [[{'name': 'valid_for_children', 'score': 1}],\n",
      "                                  [{'name': 'valid_for_children',\n",
      "                                    'score': 0}]]},\n",
      " 'evaluator_context': {'results': [[{'name': 'context_precision',\n",
      "                                     'score': 0.9999999999}],\n",
      "                                   [{'name': 'context_precision',\n",
      "                                     'score': 0.9999999999}]]}}\n"
     ]
    }
   ],
   "source": [
    "QUESTIONS = [\n",
    "    \"Which is the most popular global sport?\",\n",
    "    \"Who created the Python language?\",\n",
    "]\n",
    "CONTEXTS = [\n",
    "    [\n",
    "        \"The popularity of sports can be measured in various ways, including TV viewership, social media presence, number of participants, and economic impact. Football is undoubtedly the world's most popular sport with major events like the FIFA World Cup and sports personalities like Ronaldo and Messi, drawing a followership of more than 4 billion people.\"\n",
    "    ],\n",
    "    [\n",
    "        \"Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.\"\n",
    "    ],\n",
    "]\n",
    "RESPONSES = [\n",
    "    \"Football is the most popular sport with around 4 billion followers worldwide\",\n",
    "    \"Python language was created by Guido van Rossum.\",\n",
    "]\n",
    "GROUND_TRUTHS = [\n",
    "    \"Football is the most popular sport\",\n",
    "    \"Python language was created by Guido van Rossum.\",\n",
    "]\n",
    "\n",
    "results = pipeline.run(\n",
    "    {\n",
    "        \"evaluator_context\": {\n",
    "            \"questions\": QUESTIONS,\n",
    "            \"contexts\": CONTEXTS,\n",
    "            \"ground_truths\": GROUND_TRUTHS,\n",
    "        },\n",
    "        \"evaluator_aspect\": {\n",
    "            \"questions\": QUESTIONS,\n",
    "            \"contexts\": CONTEXTS,\n",
    "            \"responses\": RESPONSES,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DeepEvalEvaluator\n",
    "\n",
    "DeepEval is a simple-to-use, open-source LLM evaluation framework. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs locally on your machine for evaluation.\n",
    "\n",
    "DeepEval gives you additional options of using models on your machine.\n",
    "https://docs.confident-ai.com/docs/metrics-introduction#using-a-custom-llm\n",
    "\n",
    "Integrate into CI/CDs.\n",
    "\n",
    "Supported Metrics:\n",
    "\n",
    "- `ANSWER_RELEVANCY`: grades how relevant the answer was to the question specified\n",
    "- `FAITHFULNESS`: grades how factual the generated response was.\n",
    "- `CONTEXTUAL_PRECISION`: grades if he answer has any additional irrelevant information for the question asked.\n",
    "- `CONTEXTUAL_RECALL`: grades how complete the generated response was for the question specified\n",
    "- `CONTEXTUAL_RELEVANCE`: grades how relevant provided context was for the question specified\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "1. You can use the `DeepEvalEvaluator` while providing correct `metric_params` for the metric you are using.\n",
    "2. Run the `DeepEvalEvaluator`, either on its own or in a pipeline, by providing the expected input for the metric you are using.\n",
    "\n",
    "##### Examples\n",
    "\n",
    "##### Evaluate Faithfulness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test cases...\n",
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/portalocker/utils.py:218: UserWarning: timeout has no effect in blocking mode\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Faithfulness (score: 1, threshold: 0.0, strict: False, evaluation model: gpt-4, reason: The score is 1.00 because there are no contradictions, indicating that the actual output is perfectly aligned with the information presented in the retrieval context., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: When was the Rhodes Statue built?\n",
      "  - actual output: Response for question 1\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['Context for question 1']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Faithfulness (score: 1, threshold: 0.0, strict: False, evaluation model: gpt-4, reason: The score is 1.00 because there are no contradictions present, indicating perfect alignment between the actual output and the retrieval context., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Where is the Pyramid of Giza\n",
      "  - actual output: Response for question 2\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['Context for question 2']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "FaithfulnessMetric: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/solomon/Documents/projects/AI/learning_haystack/env/lib/python3.11/site-packages/portalocker/utils.py:218: UserWarning: timeout has no effect in blocking mode\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack_integrations.components.evaluators.deepeval import (\n",
    "    DeepEvalEvaluator,\n",
    "    DeepEvalMetric,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline()\n",
    "evaluator = DeepEvalEvaluator(\n",
    "    metric=DeepEvalMetric.FAITHFULNESS, metric_params={\"model\": \"gpt-4\"}\n",
    ")\n",
    "pipeline.add_component(\"evaluator\", evaluator)\n",
    "\n",
    "results = pipeline.run(\n",
    "    {\n",
    "        \"evaluator\": {\n",
    "            \"questions\": [\n",
    "                \"When was the Rhodes Statue built?\",\n",
    "                \"Where is the Pyramid of Giza\",\n",
    "            ],\n",
    "            \"contexts\": [[\"Context for question 1\"], [\"Context for question 2\"]],\n",
    "            \"responses\": [\"Response for question 1\", \"Response for question 2\"],\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Evaluation Pipelines in Haystack\n",
    "\n",
    "Statistical Evaluation compares ground truth labels with pipeline predictions, typically using metrics such as precision or recall.\n",
    "\n",
    "Here the ground truth labels of expected answers are compared to the pipeline's prediction. Mostly use this with Extractive Models. For assessing answers generated by an LLM, it is recommended we use the model-based evaluation instead, as it can incorporate measures of semantic similarity or coherence and is better suited to evaluate predictions that might differe in wording from the ground truth labels\n",
    "\n",
    "##### Statistical Evalution of Retrieved Documents\n",
    "\n",
    "##### DocumentRecallEvaluator\n",
    "\n",
    "Recall measures how often the correct document was among the retrieved documents over a set of queries.\n",
    "\n",
    "The evaluator checks how many of the ground truth documents were retrieved.\n",
    "\n",
    "Modes\n",
    "\n",
    "- `RecallMode.SINGLE_HIT`: means that any of the ground truth documents need to be retrieved to count as a correct retrieval with a recall score of 1. A single retrieved document can achieve the full score.\n",
    "\n",
    "- `RecallMode.MULTI_HIT`: means that all of the ground truth documents need to be retrieved to count as a correct retrieval with a recall score of 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "On its own\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from haystack import Document\n",
    "from haystack.components.evaluators import DocumentRecallEvaluator\n",
    "from haystack.components.evaluators.document_recall import RecallMode\n",
    "\n",
    "\n",
    "evaluator = DocumentRecallEvaluator(mode=RecallMode.SINGLE_HIT)\n",
    "result = evaluator.run(\n",
    "    ground_truth_documents=[\n",
    "        [Document(content=\"France\")], [Document(content=\"9th century\")]\n",
    "    ],\n",
    "    retrieved_documents=[\n",
    "        [Document(content=\"France\")], [Document(content=\"9th century\"), Document(content=\"10th century\")]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result['individual_scores'])\n",
    "print(result['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Pipeline\n",
    "\n",
    "In this example, we use a `DocumentRecallEvaluator` and a `DocumentMRREvaluator` in a pipeline to evaluate two answers and compare them to ground truth answers. Running a pipeline instead of the individual components simplifies calculating more than one metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0]\n",
      "[1.0, 1.0]\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from haystack import Document, Pipeline\n",
    "from haystack.components.evaluators import DocumentMRREvaluator, DocumentRecallEvaluator\n",
    "\n",
    "pipeline = Pipeline()\n",
    "mrr_evaluator = DocumentMRREvaluator()\n",
    "recall_evaluator = DocumentRecallEvaluator()\n",
    "\n",
    "pipeline.add_component(\"mrr_evaluator\", mrr_evaluator)\n",
    "pipeline.add_component(\"recall_evaluator\", recall_evaluator)\n",
    "\n",
    "ground_truth_documents = [\n",
    "    [Document(content=\"France\")], \n",
    "    [Document(content=\"9th century\"), Document(content=\"9th\")],\n",
    "]\n",
    "\n",
    "retrieved_documents = [\n",
    "    [Document(content=\"France\")],\n",
    "    [Document(content=\"9th century\"), Document(content=\"10th century\"), Document(content=\"9th\")]\n",
    "]\n",
    "\n",
    "result = pipeline.run({\n",
    "    \"mrr_evaluator\": {\n",
    "        \"ground_truth_documents\": ground_truth_documents,\n",
    "        \"retrieved_documents\": retrieved_documents,\n",
    "    },\n",
    "    \"recall_evaluator\": {\n",
    "        \"ground_truth_documents\": ground_truth_documents,\n",
    "        \"retrieved_documents\": retrieved_documents\n",
    "    }\n",
    "})\n",
    "\n",
    "for evaluator in result:\n",
    "    print(result[evaluator]['individual_scores'])\n",
    "    \n",
    "for evaluator in result:\n",
    "    print(result[evaluator]['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### DocumentMRREvaluator (Mean Reciprocal Rank)\n",
    "\n",
    "In contrast to the recall metric, mean reciprocal rank takes the position of the top correctly retrieved documents (the \"rank\") into account. \n",
    "\n",
    "It checks at what rank ground truth documents appear in the list of retrieved documents. The metric is called mean reciprocal rank (MRR).\n",
    "\n",
    "A higher mean reciprocal rank is better and indicates that relevant documents appear at an earlier position in the list of retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "On its own\n",
    "\n",
    "The example below evaluates documents retrieved for two queries. The first query, there is one ground truth document and one retrieved document.\n",
    "\n",
    "The second query, there are two ground truth documents and three retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from haystack import Document\n",
    "from haystack.components.evaluators import DocumentMRREvaluator\n",
    "\n",
    "evaluator = DocumentMRREvaluator()\n",
    "result = evaluator.run(\n",
    "    ground_truth_documents=[ \n",
    "                    [Document(content=\"France\")] ,\n",
    "                    [Document(content=\"9th century\"), Document(content=\"9th\")]\n",
    "    ],\n",
    "    retrieved_documents=[ \n",
    "                    [Document(content=\"France\")] ,\n",
    "                    [Document(content=\"9th century\"), Document(content=\"10th\"), Document(content=\"9th\")]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result['individual_scores'])\n",
    "print(result['score'])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Pipeline\n",
    "\n",
    "Same as the `DocumentRecallEvaluator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrr_evaluator [1.0, 1.0]\n",
      "recall_evaluator [1.0, 1.0]\n",
      "\n",
      "\n",
      "mrr_evaluator 1.0\n",
      "recall_evaluator 1.0\n"
     ]
    }
   ],
   "source": [
    "from haystack import Document, Pipeline\n",
    "from haystack.components.evaluators import DocumentMRREvaluator, DocumentRecallEvaluator\n",
    "\n",
    "pipeline = Pipeline()\n",
    "mrr_evaluator = DocumentMRREvaluator()\n",
    "recall_evaluator = DocumentRecallEvaluator()\n",
    "\n",
    "pipeline.add_component(\"mrr_evaluator\", mrr_evaluator)\n",
    "pipeline.add_component(\"recall_evaluator\", recall_evaluator)\n",
    "\n",
    "ground_truth_documents = [\n",
    "    [Document(content=\"France\")], \n",
    "    [Document(content=\"9th century\"), Document(content=\"9th\")],\n",
    "]\n",
    "\n",
    "retrieved_documents = [\n",
    "    [Document(content=\"France\")],\n",
    "    [Document(content=\"9th century\"), Document(content=\"10th century\"), Document(content=\"9th\")]\n",
    "]\n",
    "\n",
    "result = pipeline.run({\n",
    "    \"mrr_evaluator\": {\n",
    "        \"ground_truth_documents\": ground_truth_documents,\n",
    "        \"retrieved_documents\": retrieved_documents,\n",
    "    },\n",
    "    \"recall_evaluator\": {\n",
    "        \"ground_truth_documents\": ground_truth_documents,\n",
    "        \"retrieved_documents\": retrieved_documents\n",
    "    }\n",
    "})\n",
    "\n",
    "for evaluator in result:\n",
    "    print(evaluator, result[evaluator]['individual_scores'])\n",
    "    \n",
    "print('\\n')\n",
    "for evaluator in result:\n",
    "    print(evaluator, result[evaluator]['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### DocumentMAPEvaluator (Mean Average Precision)\n",
    "\n",
    "This component can be used to evaluate documents, a higher mean average precision is better, indicating that the list of retrieved documents contains many relevant documents and only a few non-relevant documents or none at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "On its own\n",
    "\n",
    "Showing for two queries, the first one has one ground truth and one retrieved document.\n",
    "The other query has 2 ground truths, and 3 retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.8333333333333333]\n",
      "0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "from haystack import Document\n",
    "from haystack.components.evaluators import DocumentMAPEvaluator\n",
    "\n",
    "evaluator = DocumentMAPEvaluator()\n",
    "result = evaluator.run(\n",
    "    ground_truth_documents=[\n",
    "        [Document(content=\"France\")],\n",
    "        [Document(content=\"9th century\"), Document(content=\"9th\")],\n",
    "    ],\n",
    "    retrieved_documents=[\n",
    "        [Document(content=\"France\")],\n",
    "        [Document(content=\"9th century\"), Document(content=\"10th century\"), Document(content=\"9th\")]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result['individual_scores'])\n",
    "print(result['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrr_evaluator [1.0, 1.0]\n",
      "map_evaluator [1.0, 0.8333333333333333]\n",
      "\n",
      "\n",
      "mrr_evaluator 1.0\n",
      "map_evaluator 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "from haystack import Document, Pipeline\n",
    "from haystack.components.evaluators import DocumentMRREvaluator, DocumentMAPEvaluator\n",
    "\n",
    "pipeline = Pipeline()\n",
    "mrr_evaluator = DocumentMRREvaluator()\n",
    "map_evaluator = DocumentMAPEvaluator()\n",
    "pipeline.add_component(\"mrr_evaluator\", mrr_evaluator)\n",
    "pipeline.add_component(\"map_evaluator\", map_evaluator)\n",
    "\n",
    "ground_truth_documents = [\n",
    "    [Document(content=\"France\")],\n",
    "    [Document(content=\"9th century\"), Document(content=\"9th\")]\n",
    "]\n",
    "retrieved_documents = [\n",
    "    [Document(content=\"France\")],\n",
    "    [Document(content=\"9th century\"), Document(content=\"10th century\"), Document(content=\"9th\")]\n",
    "]\n",
    "\n",
    "result = pipeline.run({\n",
    "    \"mrr_evaluator\": {\n",
    "        \"ground_truth_documents\": ground_truth_documents,\n",
    "        \"retrieved_documents\": retrieved_documents,\n",
    "    },\n",
    "    \"map_evaluator\": {\n",
    "        \"ground_truth_documents\": ground_truth_documents,\n",
    "        \"retrieved_documents\": retrieved_documents,\n",
    "    }\n",
    "})\n",
    "\n",
    "for evaluator in result:\n",
    "    print(evaluator, result[evaluator]['individual_scores'])\n",
    "print('\\n')\n",
    "for evaluator in result:\n",
    "    print(evaluator, result[evaluator]['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Evalution of Extracted or Generated Answers\n",
    "\n",
    "##### AnswerExactMatchEvaluator\n",
    "\n",
    "This component checks character by character whether a predicted answer exactly matches the ground truth answer. This metric is called exact match.\n",
    "\n",
    "This is useful for evaluating an extractive question answering pipeline against ground truth labels. \n",
    "\n",
    "`AnswerExactMatchEvaluator` checks whether a predicted answer exactly matches the ground truth answer. It is not suited to evaluate answers generated by LLMs. use `FaithfulnessEvaluator` or `SASEvaluator` instead.\n",
    "\n",
    "One predicted answer is compared to one ground truth answer at a time.\n",
    "\n",
    "If matches are not same, the value will be 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "On its own\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.evaluators import AnswerExactMatchEvaluator\n",
    "\n",
    "evaluator = AnswerExactMatchEvaluator()\n",
    "result = evaluator.run(\n",
    "    ground_truth_answers=[\"Berlin\", \"Paris\"],\n",
    "    predicted_answers=[\"Berlin\", \"Lyon\"]\n",
    ")\n",
    "\n",
    "print(result['individual_scores'])\n",
    "print(result['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "em_evaluator [1, 0]\n",
      "sas_evaluator [0.9999999403953552, 0.5174765586853027]\n",
      "\n",
      "\n",
      "em_evaluator 0.5\n",
      "sas_evaluator 0.758738249540329\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.evaluators import AnswerExactMatchEvaluator\n",
    "\n",
    "# SASEvaluator uses a cross-encoder model\n",
    "from haystack.components.evaluators import SASEvaluator\n",
    "\n",
    "pipeline = Pipeline()\n",
    "em_evaluator = AnswerExactMatchEvaluator()\n",
    "sas_evaluator = SASEvaluator()\n",
    "pipeline.add_component(\"em_evaluator\", em_evaluator)\n",
    "pipeline.add_component(\"sas_evaluator\", sas_evaluator)\n",
    "\n",
    "ground_truth_answers = [\"Berlin\", \"Paris\"]\n",
    "predicted_answers = [\"Berlin\", \"Lyon\"]\n",
    "\n",
    "result = pipeline.run({\n",
    "    \"em_evaluator\": {\n",
    "        \"ground_truth_answers\": ground_truth_answers,\n",
    "        \"predicted_answers\": predicted_answers\n",
    "    },\n",
    "    \"sas_evaluator\": {\n",
    "        \"ground_truth_answers\": ground_truth_answers,\n",
    "        \"predicted_answers\": predicted_answers,\n",
    "    },\n",
    "})\n",
    "\n",
    "for evaluator in result:\n",
    "    print(evaluator, result[evaluator]['individual_scores'])\n",
    "print('\\n')\n",
    "for evaluator in result:\n",
    "    print(evaluator, result[evaluator]['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Practical Examples\n",
    "\n",
    "We would evaluate RAG pipelines both with model-based and statistical metrics available in Haystack evaluation offering.\n",
    "\n",
    "1. Build a pipeline that answers medical questions based on PubMed data\n",
    "2. Build an evaluation pipeline that makes use of some metrics like Document MRR and Answer Faithfulness\n",
    "3. Run the RAG pipeline and evaluate the output with our evaluation pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
